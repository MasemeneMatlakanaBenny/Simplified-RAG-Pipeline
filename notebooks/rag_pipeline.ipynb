{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6014b010-86c4-46f8-90c8-8877ea1f2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btlak\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings,ChatHuggingFace,HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6252f8-c3a8-4cd5-875b-4dfbd954bf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'python_introduction.txt', 'author': 'Masemene Matlakana Benny', 'date': '13/01/2026'}, page_content='Completing the RAG Pipeline')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"Completing the RAG Pipeline\",\n",
    "    metadata={\n",
    "        \"source\":\"python_introduction.txt\",\n",
    "        \"author\":\"Masemene Matlakana Benny\",\n",
    "        \"date\":\"13/01/2026\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6372200-d769-47a0-b6a8-5974e652b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(\"python_introduction.txt\")\n",
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8133592-cf29-4e11-a68f-59523573698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text splitting:\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\"\", \" \"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765d2972-589c-4713-b0bb-ef300aeece44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the chunks:\n",
    "chunks=text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8eb3ec9-e717-43e3-99b0-682e606137a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the embedding:\n",
    "embedding=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfef775f-5eb7-4043-b490-4dd23bb5baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2a0a96eb0e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the vector store:\n",
    "vector_store=Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467dbd04-41e8-4c5a-8f29-a6ecc81ba476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002A0A96EB0E0>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the retriever:\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwags={\"k\":2}\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27e7cbe-ae9a-4c0a-9297-00faf94fc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a question-answering assistant.\n",
    "\n",
    "    RULES:\n",
    "    - Answer using ONLY the context below.\n",
    "    - If the answer is not in the context, respond EXACTLY with:\n",
    "      \"No response to the question.\"\n",
    "    - Return ONLY the final answer.\n",
    "    - Do NOT explain your reasoning.\n",
    "    - Do NOT include thoughts, analysis, tags like <think>.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c52bba-611d-4895-b987-5198cdd1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868553fe-8d76-4496-8849-e7b7b898afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='deepseek-ai/DeepSeek-R1-0528', provider='auto', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='deepseek-ai/DeepSeek-R1-0528', client=<InferenceClient(model='deepseek-ai/DeepSeek-R1-0528', timeout=120)>, async_client=<InferenceClient(model='deepseek-ai/DeepSeek-R1-0528', timeout=120)>, task='text-generation'), model_id='deepseek-ai/DeepSeek-R1-0528', temperature=0.8, frequency_penalty=1.03, top_p=0.95, max_tokens=512, model_kwargs={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model=ChatHuggingFace(llm=llm_endpoint)\n",
    "chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46aa3bf2-3bce-4b04-9262-8da65fc6c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain=({\n",
    "    \"context\":retriever,\n",
    "    \"question\":RunnablePassthrough()\n",
    "\n",
    "}\n",
    "| prompt\n",
    "| chat_model\n",
    "| StrOutputParser()\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990fb007-bcd3-40b7-8ea2-947410c3b2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nWe are given a context that includes several documents, but note that all the page_content entries are either \"Completing the RAG Pipeline\" or \"RAG 03\". There is no mention of what Python is used for in the provided context.\\n\\nTherefore, according to the rules, if the answer is not in the context, we must respond exactly with: \"No response to the question.\"\\n\\nWe are to return ONLY the final answer without any explanation, thoughts, or additional text.\\n</think>\\nNo response to the question.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"what is Python used for\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768ca99-17ee-4908-9ff9-ad3f87fdbd24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
